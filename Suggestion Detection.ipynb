{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCWSEtNeGMsN"
   },
   "source": [
    "---\n",
    "\n",
    "This involves tasks for feature engineering, training and evaluating a classifier for suggestion detection. We will work with the data from SemEval-2019 Task 9 subtask A to classify whether a piece of text contains a suggestion or not. \n",
    "\n",
    "Download train.csv, test_seen.csv and test_unseen.csv from the [Github](https://github.com/sharduls007/Assignment_2_CT5120) or uncomment the code cell below to get the data as a comma-separated values (CSV) file. The CSV file contains a header row followed by 5,440 rows in train.csv and 1,360 rows in test_seen.csv spread across 3 columns of data. Each row of data contains a unique id, a piece of text and a label assigned by an annotator. A label of $1$ indicates that the given text contains a suggestion while a label of $0$ indicates that the text does not contain a suggestion.\n",
    "\n",
    "You can find more details about the dataset in Sections 1, 2, 3 and 4 of [SemEval-2019 Task 9: Suggestion Mining from Online Reviews and Forums\n",
    "](https://aclanthology.org/S19-2151/).\n",
    "\n",
    "We will be using test_seen.csv for benchmarking our model, hence it has label. On the other hand, test_unseen is used for [Kaggle](https://www.kaggle.com/competitions/nlp2022ct5120suggestionmining/overview) competition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ShQ2lPxmPfA4",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "df651146-abe3-4d3b-8960-23eb1d2b977b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "curl: (35) schannel: next InitializeSecurityContext failed: Unknown error (0x80092012) - The revocation function was unable to check revocation for the certificate.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "curl: (35) schannel: next InitializeSecurityContext failed: Unknown error (0x80092012) - The revocation function was unable to check revocation for the certificate.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "curl: (35) schannel: next InitializeSecurityContext failed: Unknown error (0x80092012) - The revocation function was unable to check revocation for the certificate.\n"
     ]
    }
   ],
   "source": [
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/train.csv\" > train.csv\n",
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_seen.csv\" > test.csv\n",
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_unseen.csv\" > test_unseen.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5x0c38rCGk23",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file.\n",
    "train_df = pd.read_csv('train.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "test_df = pd.read_csv('test_seen.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "# Store the data as a list of tuples where the first item is the text\n",
    "# and the second item is the label.\n",
    "train_texts, train_labels = train_df[\"text\"].to_list(), train_df[\"label\"].to_list() \n",
    "test_texts, test_labels = test_df[\"text\"].to_list(), test_df[\"label\"].to_list() \n",
    "\n",
    "# Check that training set and test set are of the right size.\n",
    "assert len(test_texts) == len(test_labels) == 1360\n",
    "assert len(train_texts) == len(train_labels) == 5440"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_Scj45oSpdQ"
   },
   "source": [
    "---\n",
    "\n",
    "## 1: Data Pre-processing \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Pd8ed8NdlB_"
   },
   "source": [
    "\n",
    "---\n",
    ">We can preprocess our text by eliminating punctuation, transforming it to lower case, stripping any urls or links, and tokenizing each word in a given sentence. To maintain consistency in the word count, we must convert all text to lower case; this is one of the simplest and most successful forms of text preparation. It is applicable to the majority of text mining and NLP problems and can be useful when the dataset is small. It also considerably improves the consistency of predicted output. Then we remove the punctuation from the text; punctuation removal is the process of deleting characters and text fragments that may interfere with text analysis. It is one of the most crucial stage in text preprocessing.\n",
    "On our text, then we use tokenization. This separates the texts into smaller sub-texts (tokens), allowing for better generalization of the relationship between the texts and the labels. This determines the dataset's \"vocabulary\" (set of unique tokens present in the data). Finally, we can get rid of the stop words. We remove the low-level information from our text by deleting these terms, allowing us to focus on the crucial information.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Jb7i3Le4aSYM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "train_text = train_df.text\n",
    "test_text = test_df.text\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def preprocess(text):  \n",
    "    \n",
    "    # removal of extra spaces\n",
    "    regex_pat = re.compile(r'\\s+')\n",
    "    text_space = text.str.replace(regex_pat, ' ')\n",
    "\n",
    "    # removal of links\n",
    "    url =  re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|' '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    texts = text.str.replace(url, '')\n",
    "    \n",
    "    # removal of punctuations\n",
    "    newtext = texts.str.replace(\"[^a-zA-Z1-10]\", \" \")\n",
    "    \n",
    "    # Converting text to lower case\n",
    "    text_lower = newtext.str.lower()\n",
    "    \n",
    "    # tokenizing\n",
    "    tokenized_text = text_lower.apply(lambda x: x.split())\n",
    "    # removal of stopwords\n",
    "    tokenized_text=  tokenized_text.apply(lambda x: [item for item in x if item not in stopwords])\n",
    "    \n",
    "    for i in range(len(tokenized_text)):\n",
    "        tokenized_text[i] = ' '.join(tokenized_text[i])\n",
    "        texts_p= tokenized_text\n",
    "    \n",
    "    return texts_p\n",
    "\n",
    "processed_texts = preprocess(train_text)   \n",
    "train_df['processed_text'] = processed_texts\n",
    "\n",
    "processed_texts = preprocess(test_text)  \n",
    "test_df['processed_text'] = processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.16'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IUJunnfXItQ"
   },
   "source": [
    "---\n",
    "\n",
    "## 2: Feature Engineering (I) - TF-IDF as features\n",
    "\n",
    "The raw counts of words and `tf-idf` scores can be useful features for a classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3gDsfB8xTGMg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5544117647058824"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Calculate tf-idf scores for the words in the training set.\n",
    "tfIdfTransformer = TfidfTransformer(use_idf=True)\n",
    "countVectorizer = CountVectorizer()\n",
    "wordCount_train = countVectorizer.fit_transform(train_df.processed_text)\n",
    "newTfIdf_train = tfIdfTransformer.fit_transform(wordCount_train)\n",
    "\n",
    "wordCount_test = countVectorizer.transform(test_df.processed_text)\n",
    "newTfIdf_test = tfIdfTransformer.transform(wordCount_test)\n",
    "\n",
    "# Train a Naïve Bayes classifier using the tf-idf scores for words as features.\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(newTfIdf_train.toarray(), train_labels)\n",
    "\n",
    "\n",
    "# Predict on the test set.\n",
    "predictions = []    # save your predictions on the test set into this list\n",
    "\n",
    "predictions = clf.predict(newTfIdf_test.toarray())\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "\n",
    "  assert len(labels) == len(predictions)\n",
    "  \n",
    "  correct = 0\n",
    "  for label, prediction in zip(labels, predictions):\n",
    "    if label == prediction:\n",
    "      correct += 1 \n",
    "  \n",
    "  score = correct / len(labels)\n",
    "  return score\n",
    "\n",
    "# Calculate accuracy score for the classifier using tf-idf features.\n",
    "accuracy(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDx_M2aTIncl"
   },
   "source": [
    "---\n",
    "\n",
    "## 3: Evaluation Metrics \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8jDzSU86xI1"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "> Standard accuracy is defined as the ratio of correct classifications to the number of classifications done. Accuracy can be a useful measure if we have the same number of samples per class but if we have an imbalanced set of samples accuracy isn't useful at all. Even more so, a test can have a high accuracy but actually perform worse than a test with a lower accuracy.\n",
    "We can make use of other evaluation metrics such as precision which measures that among the cases predicted to be positive, how much percentage of them are really positive. Also recall which measures how much percentage of real positive cases are correctly identified. And finally F1 score that is created to have a balanced metric between recall and precision. It is the Harmonic mean of recall and precision.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ozD4SyyRDL3"
   },
   "source": [
    "In the code cell below, write an implementation of the evaluation metric you defined above. Please write your own implementation from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UkUX5K0oMhKI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3964143426294821"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(labels, predictions):\n",
    "\n",
    "\n",
    "  # check that labels and predictions are of same length\n",
    "  assert len(labels) == len(predictions)\n",
    "\n",
    "  score = 0.0\n",
    "\n",
    "#Defining a confusion matrix to calculate the F1-score\n",
    "  def cf(labels, predictions):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for x, y in zip(labels, predictions):\n",
    "        if x == 1 and y == 1:\n",
    "            tp +=1\n",
    "    for x, y in zip(labels, predictions):\n",
    "        if x == 0 and y == 0:\n",
    "            tn +=1\n",
    "    for x, y in zip(labels, predictions):\n",
    "        if x == 0 and y == 1:\n",
    "            fp +=1\n",
    "    for x, y in zip(labels, predictions):\n",
    "        if x == 1 and y == 0:\n",
    "            fn +=1\n",
    "    \n",
    "    prec = tp/ (tp + fp)  \n",
    "    rec = tp/ (tp + fn)  \n",
    "\n",
    "    p = prec\n",
    "    r = rec\n",
    "    score = 2 * p * r/ (p + r) \n",
    "    return score\n",
    "  \n",
    "  score=cf(labels, predictions)\n",
    "\n",
    "  return score\n",
    "\n",
    "# Calculate evaluation score based on the metric of your choice\n",
    "# for the classifier trained in Task 2 using tf-idf features.\n",
    "evaluate(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22OelF89a27J"
   },
   "source": [
    "---\n",
    "\n",
    "## 4: Feature Engineering (II) - Other features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EBS0F877UyC"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "> We can implement simple characteristics that are less CPU-intensive and comparably easier to compute. We can use features such as counting the amount of words and characters in each sentence. Character length and word length are frequently found to be relevant in text datasets. Count the amount of distinct words; this feature allows us to see if there are any word repetitions in the data points. We can even calculate the puncutation marks from the original text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "u9mRku0va8kK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.471161657189277"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create your features.\n",
    "\n",
    "# count number of characters \n",
    "def count_chars(text):\n",
    "    return len(text)\n",
    "\n",
    "# count number of words \n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# count number of punctuations\n",
    "def count_punctuations(text):\n",
    "    punctuations='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    d=dict()\n",
    "    for i in punctuations:\n",
    "        d[str(i)+' count']=text.count(i)\n",
    "    return d\n",
    "\n",
    "# count number of unique words \n",
    "def count_unique_words(text):\n",
    "    return len(set(text.split()))\n",
    "            \n",
    "#applying the features to the train and test dataset\n",
    "train_df['char_count'] = train_df[\"processed_text\"].apply(lambda x:count_chars(x))\n",
    "train_df['word_count'] = train_df[\"processed_text\"].apply(lambda x:count_words(x))\n",
    "train_df['punct_count'] = train_df[\"text\"].apply(lambda x:count_punctuations(x))\n",
    "train_df['unique_count'] = train_df[\"processed_text\"].apply(lambda x:count_unique_words(x))\n",
    "\n",
    "test_df['char_count'] = test_df[\"processed_text\"].apply(lambda x:count_chars(x))\n",
    "test_df['word_count'] = test_df[\"processed_text\"].apply(lambda x:count_words(x))\n",
    "test_df['punct_count'] = test_df[\"text\"].apply(lambda x:count_punctuations(x))\n",
    "test_df['unique_count'] = test_df[\"processed_text\"].apply(lambda x:count_unique_words(x))\n",
    "\n",
    "#creating sepearte columns for each puncuation for train and test dataset\n",
    "df_punct= pd.DataFrame(list(train_df.punct_count))\n",
    "test_punct= pd.DataFrame(list(test_df.punct_count))\n",
    "train_df=pd.merge(train_df,df_punct,left_index=True, right_index=True)\n",
    "test_df=pd.merge(test_df,test_punct,left_index=True, right_index=True)\n",
    "train_df.drop(columns=['punct_count'],inplace=True)\n",
    "test_df.drop(columns=['punct_count'],inplace=True)\n",
    "\n",
    "vectorizer =  TfidfVectorizer()\n",
    "train_tf_idf_features =  vectorizer.fit_transform(train_df['processed_text']).toarray()\n",
    "test_tf_idf_features  =  vectorizer.transform(test_df['processed_text']).toarray()\n",
    "\n",
    "train_tf_idf = pd.DataFrame(train_tf_idf_features) \n",
    "test_tf_idf = pd.DataFrame(test_tf_idf_features)\n",
    "\n",
    "train_Y = train_df['label']\n",
    "test_Y = test_df['label']\n",
    "\n",
    "# Listing all features\n",
    "features = ['char_count', 'word_count','unique_count','! count', '\" count', '# count', '$ count',\n",
    "       '% count', '& count', '\\' count', '( count', ') count', '* count',\n",
    "       '+ count', ', count', '- count', '. count', '/ count', ': count',\n",
    "       '; count', '< count', '= count', '> count', '? count', '@ count',\n",
    "       '[ count', '\\ count', '] count', '^ count', '_ count', '` count',\n",
    "       '{ count', '| count', '} count', '~ count']\n",
    "# Finally merging all features with above TF-IDF. \n",
    "train = pd.merge(train_tf_idf,train_df[features],left_index=True, right_index=True)\n",
    "test  = pd.merge(test_tf_idf,test_df[features],left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "# Train a Naïve Bayes classifier using the features you defined.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train, train_Y)\n",
    "predictions = clf.predict(test)\n",
    "\n",
    "# Evaluate on the test set.\n",
    "evaluate(test_Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyDD1zFQdwCf"
   },
   "source": [
    "---\n",
    "\n",
    "## 5: Kaggle Competition \n",
    "\n",
    "Head over to https://www.kaggle.com/t/1f90b74da0b7484da9647638e22d106\n",
    "Use above classifier to predict the label for test_unseen.csv from competition page and upload the results to the leaderboard. The current baseline score is 0.36823. Make an improvement above the baseline. Please note that the evaluation metric for the competition is the f-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JaC6B824Fe0H"
   },
   "outputs": [],
   "source": [
    "# Preparing submission for Kaggle\n",
    "StudentID = \"22224253_Borole\" # Please add your student id and lastname\n",
    "test_unseen = pd.read_csv(\"test_unseen.csv\", names=['id', 'text'], header=0)\n",
    "\n",
    "unseen = test_unseen['text']\n",
    "processed_texts = preprocess(unseen)  \n",
    "test_unseen['processed_text'] = processed_texts\n",
    "test_unseen['char_count'] = test_unseen[\"processed_text\"].apply(lambda x:count_chars(x))\n",
    "test_unseen['word_count'] = test_unseen[\"processed_text\"].apply(lambda x:count_words(x))\n",
    "test_unseen['punct_count'] = test_unseen[\"text\"].apply(lambda x:count_punctuations(x))\n",
    "test_unseen['unique_count'] = test_unseen[\"processed_text\"].apply(lambda x:count_unique_words(x))\n",
    "\n",
    "testunseen_punct= pd.DataFrame(list(test_unseen.punct_count))\n",
    "test_unseen=pd.merge(test_unseen,testunseen_punct,left_index=True, right_index=True)\n",
    "test_unseen.drop(columns=['punct_count'],inplace=True)\n",
    "\n",
    "test_unseen_tfidf_features  =  vectorizer.transform(test_unseen['processed_text']).toarray()\n",
    "test_unseen_tf_idf = pd.DataFrame(test_unseen_tfidf_features)\n",
    "\n",
    "t_unseen = pd.merge(test_unseen_tf_idf,test_unseen[features],left_index=True, right_index=True)\n",
    "\n",
    "pred = clf.predict(t_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZQumdT-9yet"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "> For the test unseen dataset, I preprocessed the original text in part 2 of the assignment using my user defined function and stored it as a new column. Then, from the processed text, features such as word count, character count, punctuation count from the original text, and unique words were extracted. After obtaining our features, I used the tfidf vectoririser to weight them and then used it to predict labels using SkLearn's multinomialNB classifier. On the test unseen dataset, my mean avg f-score is 0.56529.\n",
    "We can see that my score is higher than the specified baseline model, which is '0.36823'. This improvement could be attributed to the usage of user-defined features as well as preparation of textual data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
