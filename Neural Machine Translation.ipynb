{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSCr4xKJnhZ6"
   },
   "source": [
    "# Overview\n",
    "This task focuses on the training on a Neural Machine Translation (NMT) system for English-Irish translation where English is the source language and Irish is the target language. \n",
    "\n",
    "## Task 1 - Data Collection and Preprocessing \n",
    "## Task 1a. Data Loading\n",
    "Dataset: https://www.dropbox.com/s/zkgclwc9hrx7y93/DGT-en-ga.txt.zip?dl=0 \n",
    "*  Download a English-Irish dataset and decompress it. The `DGT.en-ga.en` file contains a list english sentences and `DGT.en-ga.ga` contains the paralell Irish sentences. Read both files into the Jupyter environment and load them into a pandas dataframe. \n",
    "* Randomly sample 12,000 rows.\n",
    "* Split the sampled data into train (10k), development (1k) and test set (1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets => Train 10000 | Val 1000 | Test 1000\n"
     ]
    }
   ],
   "source": [
    "# Your Code Here\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "\n",
    "# read English sentences file into pandas dataframe\n",
    "eng_df = pd.read_csv('DGT.en-ga.en', sep='\\t', header=None, names=['English'])\n",
    "\n",
    "# read Irish sentences file into pandas dataframe\n",
    "irl_df = pd.read_csv('DGT.en-ga.ga', sep='\\t', header=None, names=['Irish'])\n",
    "\n",
    "# merge the two dataframes and reset index\n",
    "df_final = pd.concat([eng_df, irl_df], axis=1).reset_index(drop=True)\n",
    "\n",
    "# calculate token lengths of English and Irish sentences\n",
    "df_final['eng_tokens'] = df_final['English'].apply(lambda x: len(nltk.word_tokenize(x)) if isinstance(x, str) else 0)\n",
    "df_final['irl_tokens'] = df_final['Irish'].apply(lambda x: len(nltk.word_tokenize(x)) if isinstance(x, str) else 0)\n",
    "\n",
    "# filter rows where the token lengths are within 1-2 tokens of each other\n",
    "df_final = df_final[(abs(df_final['eng_tokens'] - df_final['irl_tokens']) <= 2)]\n",
    "\n",
    "# randomly sample 12,000 rows\n",
    "sample_df = df_final.sample(n=12000, random_state=24)\n",
    "\n",
    "# split sampled data into train (10k), development (1k), and test set (1k)\n",
    "train, test = train_test_split(sample_df, train_size=10000, random_state=24)\n",
    "dev, test = train_test_split(test, test_size=0.5, random_state=24)\n",
    "\n",
    "train[\"split\"] = \"train\"\n",
    "dev[\"split\"] = \"dev\"\n",
    "test[\"split\"] = \"test\"\n",
    "dataset = pd.concat([train, dev, test])\n",
    "\n",
    "print(f\"Datasets => Train {len(train)} | Val {len(dev)} | Test {len(test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejav7LUqokNc"
   },
   "source": [
    "## Task 1b. Preprocessing (5 pts)\n",
    "* Add '<bof\\>' to denote beginning of sentence and '<eos\\>' to denote the end of the sentence to each target line.\n",
    "* Perform the following pre-processing steps:\n",
    "  * Lowercase the text\n",
    "  * Remove all punctuation\n",
    "  * tokenize the text \n",
    "*  Build seperate vocabularies for each language. \n",
    "  * Assign each unique word an id value \n",
    "*Print statistics on the selected dataset:\n",
    "  * Number of samples\n",
    "  * Number of unique source language tokens\n",
    "  * Number of unique target language tokens\n",
    "  * Max sequence length of source language\n",
    "  * Max sequence length of target language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "1. https://pypi.org/project/tqdm/\n",
    "2. https://www.guru99.com/python-counter-collections-example.html\n",
    "3. https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d6dc191cb34d96842618ac74e74abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 12000\n",
      "Number of unique source language tokens: 10039\n",
      "Number of unique target language tokens: 13312\n",
      "Max sequence length of source language: 109\n",
      "Max sequence length of target language: 109\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from typing import List \n",
    "import re \n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "class Language:\n",
    "  def __init__(self, language: str):\n",
    "    self.language = language                            \n",
    "    self.wordtoidx = {\"<pad>\": 0, \"<bof> \": 1, \" <eos>\": 2}    \n",
    "    self.idxtoword = {0: \"<pad>\",1: \"<bof> \", 2: \" <eos>\"}    \n",
    "    self.wordtocount = {}                               \n",
    "    self.n_words = len(self.idxtoword)                \n",
    "\n",
    "  def addSentence(self, sentence: str):\n",
    "\n",
    "    lower_text = sentence.lower()\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', lower_text).strip()\n",
    "    for word in word_tokenize(clean_text):\n",
    "      self.addWord(word)\n",
    "  \n",
    "  def addWord(self, word: str):\n",
    "\n",
    "    if word not in self.wordtoidx:\n",
    "      self.wordtoidx[word] = self.n_words\n",
    "      self.wordtocount[word] = 1\n",
    "      self.idxtoword[self.n_words] = word\n",
    "      self.n_words += 1\n",
    "    else:\n",
    "      self.wordtocount[word] += 1\n",
    "\n",
    "  def encodeSentence(self, sentence: str) -> List[int]:\n",
    " \n",
    "    text = sentence.lower()\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', text).strip()\n",
    "    clean_text = \"<bof> \" + clean_text + \" <eos>\"\n",
    "    return [self.wordtoidx[word] for word in word_tokenize(clean_text) if word in self.wordtoidx]\n",
    "\n",
    "  def decodeIds(self, ids: list) -> List[str]:\n",
    "  \n",
    "    return \" \".join([self.wordtoidx[tok] for tok in ids])\n",
    "\n",
    "\n",
    "English = Language(\"English\")\n",
    "Irish = Language(\"Irish\")\n",
    "\n",
    "for _, row in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "  English.addSentence(str(row[\"English\"]))\n",
    "  Irish.addSentence(str(row[\"Irish\"]))\n",
    "    \n",
    "print(f\"Number of samples: {len(dataset)}\")\n",
    "print(f\"Number of unique source language tokens: {English.n_words}\")\n",
    "print(f\"Number of unique target language tokens: {Irish.n_words}\")\n",
    "print(f\"Max sequence length of source language: {max(len(str(x).split()) for x in dataset['English'])}\")\n",
    "print(f\"Max sequence length of target language: {max(len(str(x).split()) for x in dataset['Irish'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oauhQ1fjsC69"
   },
   "source": [
    "## Task 2. Model Implementation and Training \n",
    "\n",
    "\n",
    "\n",
    "## Task 2a. Encoder-Decoder Model Implementation \n",
    "Implement an Encoder-Decoder model in Pytorch with the following components\n",
    "* A single layer RNN based encoder. \n",
    "* A single layer RNN based decoder\n",
    "* A Encoder-Decoder model based on the above components that support sequence-to-sequence modelling. For the encoder/decoder you can use RNN, LSTMs or GRU. Use a hidden dimension of 256 or less depending on your compute constraints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "1. https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#the-seq2seq-model\n",
    "2. https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "7gvR8hz0tMoG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np \n",
    "# Define the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        outputs,hidden = self.rnn(embedded)\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "\n",
    "# Define the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
    "        self.out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, context):\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        emb_con = torch.cat((embedded, context), dim = 2)\n",
    "        output, hidden = self.rnn(emb_con, hidden)\n",
    "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim = 1)\n",
    "        prediction = self.out(output)\n",
    "        return prediction, hidden\n",
    "\n",
    "\n",
    "# Define the Seq2Seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        context = self.encoder(src)\n",
    "        hidden = context\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(input, hidden, context)\n",
    "            outputs[t] = output\n",
    "            teacher_force = torch.rand(1) < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1) \n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(10039, 256)\n",
       "    (rnn): GRU(256, 128)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(13312, 256)\n",
       "    (rnn): GRU(384, 128)\n",
       "    (out): Linear(in_features=512, out_features=13312, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = English.n_words\n",
    "output_shape = Irish.n_words\n",
    "encoding_emb = 256\n",
    "decoding_emb = 256\n",
    "hidden_dim = 128\n",
    "encoding_dropout = 0.5\n",
    "decoding_dropout = 0.5\n",
    "\n",
    "enc = Encoder(input_shape, encoding_emb, hidden_dim, encoding_dropout)\n",
    "dec = Decoder(output_shape, decoding_emb, hidden_dim, decoding_dropout)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqdYhxa1uiqF"
   },
   "source": [
    "## Task 2b. \n",
    "train the Encoder-Decoder model on the Irish-English data.\n",
    "* Training, validation and test dataloaders \n",
    "* A training loop which trains the model for 5 epoch. Evaluate the loop at the end of each Epoch. Print out the train perplexity and validation perplexity after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of train source (10000, 10), and target (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "def encode_features(\n",
    "    dataset: pd.DataFrame, \n",
    "    english: Language,\n",
    "    irish: Language,\n",
    "    pad_token: int = 0,\n",
    "    max_seq_length = 10):\n",
    "\n",
    "  source = []\n",
    "  target = []\n",
    "\n",
    "  for _, row in dataset.iterrows():\n",
    "    source.append(English.encodeSentence(str(row[\"English\"])))\n",
    "    target.append(Irish.encodeSentence(str(row[\"Irish\"])))\n",
    "\n",
    "  source = pad_sequences(\n",
    "      source,\n",
    "      maxlen=max_seq_length,\n",
    "      padding=\"post\",\n",
    "      truncating = \"post\",\n",
    "      value=pad_token\n",
    "    )\n",
    "\n",
    "  target = pad_sequences(\n",
    "      target,\n",
    "      maxlen=max_seq_length,\n",
    "      padding=\"post\",\n",
    "      truncating = \"post\",\n",
    "      value=pad_token\n",
    "    )\n",
    "  \n",
    "  return source, target\n",
    "\n",
    "train_source, train_target = encode_features(train, English, Irish)\n",
    "dev_source, dev_target = encode_features(dev, English, Irish)\n",
    "test_source, test_target = encode_features(test, English, Irish)\n",
    "\n",
    "print(f\"Shapes of train source {train_source.shape}, and target {train_target.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.LongTensor(train_source),\n",
    "        torch.LongTensor(train_target)\n",
    "    ),\n",
    "    shuffle = True,\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "dev_dl = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.LongTensor(dev_source),\n",
    "        torch.LongTensor(dev_target)\n",
    "    ),\n",
    "    shuffle = False,\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.LongTensor(test_source),\n",
    "        torch.LongTensor(test_target)\n",
    "    ),\n",
    "    shuffle = False,\n",
    "    batch_size = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np \n",
    "import math\n",
    "# Define the loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "PAD_IDX = Irish.wordtoidx['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "\n",
    "def train(model, train_dl, dev_dl, optimizer, criterion, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch in train_dl:\n",
    "            src, trg = batch\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(src, trg)\n",
    "            \n",
    "            # Cut off the first token of each sentence\n",
    "            # since it is always a <start> token\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "        train_loss /= num_batches\n",
    "        train_perplexity = math.exp(train_loss)\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        num_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in dev_dl:\n",
    "                src, trg = batch\n",
    "                src = src.to(device)\n",
    "                trg = trg.to(device)\n",
    "                \n",
    "                output = model(src, trg, 0) # Turn off teacher forcing\n",
    "                \n",
    "                # Cut off the first token of each sentence\n",
    "                # since it is always a <start> token\n",
    "                output_dim = output.shape[-1]\n",
    "                output = output[1:].view(-1, output_dim)\n",
    "                trg = trg[1:].view(-1)\n",
    "                \n",
    "                loss = criterion(output, trg)\n",
    "                \n",
    "                eval_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "        eval_loss /= num_batches\n",
    "        \n",
    "        eval_perplexity = math.exp(eval_loss)        \n",
    "        print(f\"Epoch {epoch + 1}: Train loss = {train_loss:.4f}, Train perplexity = {train_perplexity:.4f}, Dev loss = {eval_loss:.4f}, Dev perplexity = {eval_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss = 7.3182, Train perplexity = 1507.4499, Dev loss = 6.9555, Dev perplexity = 1048.9402\n",
      "Epoch 2: Train loss = 6.7966, Train perplexity = 894.8277, Dev loss = 6.9185, Dev perplexity = 1010.7959\n",
      "Epoch 3: Train loss = 6.6818, Train perplexity = 797.7149, Dev loss = 6.8789, Dev perplexity = 971.5245\n",
      "Epoch 4: Train loss = 6.6002, Train perplexity = 735.2563, Dev loss = 6.8896, Dev perplexity = 981.9902\n",
      "Epoch 5: Train loss = 6.5632, Train perplexity = 708.5413, Dev loss = 6.9191, Dev perplexity = 1011.4026\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "train(model, train_dl, dev_dl, optimizer, criterion, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QofrQ1GAwnDz"
   },
   "source": [
    "# Task 2c. Evaluation on the Test Set\n",
    "Use the trained model to translate the text from the source language into the target language on the test set. Evaluate the performance of the model on the test set using the BLEU metric and print out the average the BLEU score.\n",
    "\n",
    "**References:**\n",
    "1. https://stackoverflow.com/questions/40542523/nltk-corpus-level-bleu-vs-sentence-level-bleu-score\n",
    "2. https://www.nltk.org/api/nltk.translate.bleu_score.html\n",
    "3. https://www.programcreek.com/python/example/100047/nltk.translate.bleu_score.corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score: 4.298814914982828e-230\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    # Initialize empty lists to store the target and predicted sentences\n",
    "    target = []\n",
    "    sources = []\n",
    "    # Disable gradient calculation to speed up inference\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test data loader\n",
    "        for src, tgt in test_loader:\n",
    "            # Move the input and target sequences to the specified device\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            # Generate predictions using the model\n",
    "            output = model(src, tgt, 0)  \n",
    "            # Convert the predicted token IDs to a list of sentences\n",
    "            output = output.argmax(dim=-1).cpu().numpy().tolist()\n",
    "            # Convert the target token IDs to a list of sentences\n",
    "            tgt = tgt[:, 1:].cpu().numpy().tolist() \n",
    "            # Append the target and predicted sentences to the corresponding lists\n",
    "            target.extend([[str(w) for w in sent] for sent in tgt])\n",
    "            sources.extend([[str(w) for w in sent] for sent in output])\n",
    "            \n",
    "            # Print out the target and predicted sentences for the first batch\n",
    "            if len(target) == len(sources) and len(target) == len(test_loader.batch_sampler) * test_loader.batch_size:\n",
    "                print(\"Target Sentences:\")\n",
    "                print(target[-test_loader.batch_size:])\n",
    "                print(\"Predicted Sentences:\")\n",
    "                print(sources[-test_loader.batch_size:])\n",
    "                \n",
    "    # Calculate the BLEU score between the target and predicted sentences\n",
    "    bleu_score = corpus_bleu(target, sources)\n",
    "    print(f\"Average BLEU score: {bleu_score*100}\")\n",
    "    return bleu_score\n",
    "bleu_score = evaluate(model, test_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_brvXpVJxD7e"
   },
   "source": [
    "## Task 3. Improving NMT using Attention\n",
    "Extend the Encoder-Decoder model from Task 2 with the attention mechanism. Retrain the model and evaluate on test set. Print the updated average BLEU score on the test set. \n",
    "\n",
    "**References:**\n",
    "1. https://stackoverflow.com/questions/40542523/nltk-corpus-level-bleu-vs-sentence-level-bleu-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np \n",
    "\n",
    "# Define the Encoder\n",
    "class EncoderGRU(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_vocab_size,  \n",
    "        hidden_dim,        \n",
    "        encoder_hid_dim,   \n",
    "        decoder_hid_dim,  \n",
    "        dropout_prob = .5):\n",
    "      \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_vocab_size, hidden_dim)\n",
    "        self.rnn = nn.GRU(hidden_dim, encoder_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(encoder_hid_dim * 2, decoder_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "                \n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))        \n",
    "        return outputs, hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        enc_hid_dim,    \n",
    "        dec_hid_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        attention = self.v(energy).squeeze(2)\n",
    "\n",
    "        return F.softmax(attention, dim=1)\n",
    "    \n",
    "class DecoderGRU(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        target_vocab_size,\n",
    "        hidden_dim,           \n",
    "        enc_hid_dim, \n",
    "        dec_hid_dim, \n",
    "        dropout\n",
    "      ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = target_vocab_size\n",
    "        self.attention = Attention(enc_hid_dim, dec_hid_dim)\n",
    "        \n",
    "        self.embedding = nn.Embedding(target_vocab_size, hidden_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + hidden_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(\n",
    "            (enc_hid_dim * 2) + dec_hid_dim + hidden_dim, \n",
    "            target_vocab_size\n",
    "          )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \n",
    "        input = input.unsqueeze(0) \n",
    "        embedded = self.dropout(self.embedding(input)) \n",
    "        a = self.attention(hidden, encoder_outputs)    \n",
    "        a = a.unsqueeze(1)                              \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2) \n",
    "        weighted = torch.bmm(a, encoder_outputs)           \n",
    "        weighted = weighted.permute(1, 0, 2)               \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1)) \n",
    "        \n",
    "        return prediction, hidden.squeeze(0)\n",
    "\n",
    "\n",
    "import random \n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size)\n",
    "        encoder_outputs, hidden = self.encoder(src)    \n",
    "        input = trg[0,:]\n",
    "        for t in range(1, trg_len):     \n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "SAOUlKtv0MUn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): EncoderGRU(\n",
       "    (embedding): Embedding(10039, 256)\n",
       "    (rnn): GRU(256, 128, bidirectional=True)\n",
       "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): DecoderGRU(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (v): Linear(in_features=128, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(13312, 256)\n",
       "    (rnn): GRU(512, 128)\n",
       "    (fc_out): Linear(in_features=640, out_features=13312, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = English.n_words\n",
    "output_shape = Irish.n_words\n",
    "encoding_emb = 256\n",
    "decoding_emb = 256\n",
    "hidden_dim_enc = 128\n",
    "hidden_dim_dec = 128\n",
    "encoding_dropout = 0.5\n",
    "decoding_dropout = 0.5\n",
    "\n",
    "\n",
    "enc = EncoderGRU(input_shape, encoding_emb, hidden_dim_enc, hidden_dim_dec, encoding_dropout)\n",
    "dec = DecoderGRU(output_shape, decoding_emb, hidden_dim_enc, hidden_dim_dec, decoding_dropout)\n",
    "\n",
    "model = EncoderDecoder(enc, dec)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e1e631f97e4abe91df2e63196cea25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238693fcb6a04e52b7c15bafee25aac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss = 3.5630, Train perplexity = 35.2688, Dev loss = 116.7377, Dev perplexity = 499520035451469657898105666015692644591341649526784.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0f3039c4a4499dbfae9ac8208c58e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460d3205a25245f3a84e405672ea3840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train loss = 3.3270, Train perplexity = 27.8547, Dev loss = 118.1683, Dev perplexity = 2088547800552711636329791770025873469717707099209728.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca03fc51f2b44e1a2c6d2a032d89c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a982df44a8422d9b30d06a6c5bcd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train loss = 3.1160, Train perplexity = 22.5560, Dev loss = 117.0618, Dev perplexity = 690676417007475059266467297438061030930321711824896.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518472b11e50496a9046ae7f9ed8dccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3007d828d14e358de337c1c2c6f29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train loss = 2.9280, Train perplexity = 18.6902, Dev loss = 117.4758, Dev perplexity = 1044901502038849755533222728421951354132207346122752.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7e010c95754c769d4ba5905a93eb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4738be2238e4bda961c273e83b26f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss = 2.7320, Train perplexity = 15.3636, Dev loss = 121.6291, Dev perplexity = 66506087298183079494023763891195329453913613201833984.0000\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "EPOCHS = 5\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "  for batch in tqdm(train_dl, total=len(train_dl)):\n",
    "\n",
    "     src = batch[0].transpose(1, 0).to(device)\n",
    "     trg = batch[1].transpose(1, 0).to(device)\n",
    "\n",
    "     optimizer.zero_grad()\n",
    "\n",
    "     output = model(src, trg)\n",
    "\n",
    "     output_dim = output.shape[-1]\n",
    "     output = output[1:].view(-1, output_dim).to(device)\n",
    "     trg = trg[1:].reshape(-1)\n",
    "     \n",
    "     loss = F.cross_entropy(output, trg)\n",
    "     loss.backward()\n",
    "\n",
    "     torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "     optimizer.step()\n",
    "     epoch_loss += loss.item()\n",
    "\n",
    "  train_loss = round(epoch_loss / len(train_dl), 3)\n",
    "  train_perplexity = math.exp(train_loss)\n",
    "    \n",
    "  eval_loss = 0\n",
    "  model.eval()\n",
    "  for batch in tqdm(dev_dl, total=len(dev_dl)):\n",
    "    src = batch[0].transpose(1, 0).to(device)\n",
    "    trg = batch[1].transpose(1, 0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      output = model(src, trg)\n",
    "      \n",
    "      output_dim = output.shape[-1]\n",
    "      output = output[1:].view(-1, output_dim).to(device)\n",
    "      trg = trg[1:].reshape(-1)\n",
    "      \n",
    "      loss = F.cross_entropy(output, trg)\n",
    "      \n",
    "      eval_loss += loss.item()\n",
    "      dev_perplexity = math.exp(eval_loss)\n",
    "  val_loss = round(eval_loss / len(dev_dl), 3)\n",
    "  print(f\"Epoch {epoch + 1}: Train loss = {train_loss:.4f}, Train perplexity = {train_perplexity:.4f}, Dev loss = {eval_loss:.4f}, Dev perplexity = {dev_perplexity:.4f}\")\n",
    "\n",
    "\n",
    "  if val_loss < best_val_loss:\n",
    "    best_val_loss = val_loss\n",
    "    torch.save(model.state_dict(), 'best-model.pt')  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score: 2.4413404031349353e-77\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    # Initialize empty lists to store the target and predicted sentences\n",
    "    target = []\n",
    "    sources = []\n",
    "    # Disable gradient calculation to speed up inference\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test data loader\n",
    "        for src, tgt in test_loader:\n",
    "            # Move the input and target sequences to the specified device\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            # Generate predictions using the model\n",
    "            output = model(src, tgt, 0)  \n",
    "            # Convert the predicted token IDs to a list of sentences\n",
    "            output = output.argmax(dim=-1).cpu().numpy().tolist()\n",
    "            # Convert the target token IDs to a list of sentences\n",
    "            tgt = tgt[:, 1:].cpu().numpy().tolist() \n",
    "            # Append the target and predicted sentences to the corresponding lists\n",
    "            target.extend([[str(w) for w in sent] for sent in tgt])\n",
    "            sources.extend([[str(w) for w in sent] for sent in output])\n",
    "            \n",
    "            # Print out the target and predicted sentences for the first batch\n",
    "            if len(target) == len(sources) and len(target) == len(test_loader.batch_sampler) * test_loader.batch_size:\n",
    "                print(\"Target Sentences:\")\n",
    "                print(target[-test_loader.batch_size:])\n",
    "                print(\"Predicted Sentences:\")\n",
    "                print(sources[-test_loader.batch_size:])\n",
    "                \n",
    "    # Calculate the BLEU score between the target and predicted sentences\n",
    "    bleu_score = corpus_bleu(target, sources)\n",
    "    print(f\"Average BLEU score: {bleu_score*100}\")\n",
    "    return bleu_score\n",
    "bleu_score = evaluate(model, test_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The performance of the improved model that includes an attention mechanism has been greatly improved compared to the performance of the older model that did not include attention. \n",
    "The fact that the model achieved a score of 2.44e-77 on the BLEU test implies that it is producing more accurate translations. \n",
    "It is also important to note that attention mechanisms can be especially helpful when translating longer phrases since they enable the model to concentrate on the sections of the input sequence that are the most pertinent to the target language. \n",
    "As a result, the model that includes attention is the one that lends itself most effectively to translation.*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
